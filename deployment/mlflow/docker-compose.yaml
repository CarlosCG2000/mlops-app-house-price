version: '3'
services:
  mlflow: # Este servicio define el contenedor que ejecutará el servidor de MLflow.
    image: ghcr.io/mlflow/mlflow:latest # Especifica la imagen de Docker a usar. En este caso, utiliza la imagen oficial más reciente de MLflow desde GitHub Container Registry (ghcr.io).
    ports: # Mapea los puertos.
      - "5555:5000" # El puerto 5000 dentro del contenedor (donde se ejecuta el servidor de MLflow) se expone al puerto 5555 de tu máquina local. Acceder al servidor de MLflow desde el navegador en http://localhost:5555.
    command: mlflow server --host 0.0.0.0 --backend-store-uri sqlite:///mlflow.db --default-artifact-root /tmp/mlruns # Este comando es lo que se ejecuta al iniciar el contenedor. ** 1 **
    container_name: mlflow-tracking-server # Asigna un nombre específico al contenedor para que sea fácil de identificar y gestionar.

# ** 1 **
# Inicia el servidor de MLflow con tres argumentos clave:
# --host 0.0.0.0: Hace que el servidor sea accesible desde cualquier dirección IP.
# --backend-store-uri sqlite:///mlflow.db: Define dónde se almacenarán los metadatos de los experimentos (parámetros, métricas y modelos). Usa una base de datos SQLite llamada mlflow.db.
# --default-artifact-root /tmp/mlruns: Especifica la ubicación donde se guardarán los "artefactos" de los experimentos (gráficos, modelos guardados, etc.). En este caso, se almacenarán en el directorio /tmp/mlruns dentro del contenedor.

# _________________ Lógica de Implementación y Futuro _________________
# Este despliegue tiene una lógica de implementación clara y marca una base para el futuro:

# Modelo de Datos Básico: Utilizar una base de datos SQLite (mlflow.db) es una solución simple y adecuada para el desarrollo y las pruebas. Sin embargo, en un entorno de producción, esta base de datos no es escalable y se almacena dentro del contenedor.

# Lógica para MLOps: Este servidor de MLflow es el componente central que permite la gestión de experimentos y modelos. Cuando tu flujo de trabajo de GitHub Actions ejecute un script de entrenamiento de un modelo, el script puede enviar los resultados (métricas, parámetros, etc.) a este servidor de MLflow. Esto crea un registro centralizado de todos tus experimentos de ML.

# _________________ Escalabilidad y Futuro _________________
# La configuración actual es para una arquitectura monolítica en una sola máquina. Para una implementación en producción, la lógica y el archivo evolucionarían de la siguiente manera:

# Escalabilidad del Backend Store: La base de datos SQLite no es adecuada para un entorno de producción con múltiples usuarios. En el futuro, la línea --backend-store-uri se modificaría para usar una base de datos escalable como PostgreSQL o MySQL.

# # Futuro: usando PostgreSQL
# services:
#   mlflow:
#     command: mlflow server --backend-store-uri postgresql://...
#   db: # Nuevo servicio de base de datos
#     image: postgres:latest
#     ...

# Gestión de Artefactos: La ubicación /tmp/mlruns es temporal. Para garantizar que los artefactos no se pierdan, se usaría un almacén de objetos persistente como Amazon S3, Azure Blob Storage o Google Cloud Storage.

# Paso a Kubernetes: La lógica que tienes en docker-compose.yaml es un excelente punto de partida para tu próximo paso: desplegar MLflow en Kubernetes. La lógica sería la misma, pero usarías un manifiesto (.yaml) para un Deployment y un Service en lugar de docker-compose.yaml. Esto te permitiría escalar el servidor de MLflow a múltiples pods y garantizar su alta disponibilidad, integrándose con el clúster que ya has configurado.

